{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ramesh/Documents/python_AI_lab/TubineDefectDector/data\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "%cd ./TubineDefectDector/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### copy files using shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_image_file(source,target):\n",
    "    assert not os.path.isabs(source)\n",
    "    try:\n",
    "        shutil.copy(source, target)\n",
    "    except IOError as e:\n",
    "        print(\"Unable to copy file. %s\" % e)\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomly split data into train and test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_images_files(filepath):\n",
    "files = os.listdir(filepath)\n",
    "for f in files:\n",
    "    if round(random.random(),1) < 0.8:\n",
    "        tgt = os.path.join('./train_files',f)\n",
    "        scr = os.path.join(imgpath,f)\n",
    "        copy_image_file(scr,tgt)\n",
    "    else:\n",
    "        tgt = os.path.join('./test_files',f)\n",
    "        scr = os.path.join(imgpath,f)\n",
    "        copy_image_file(scr,tgt)\n",
    "# set path and list filenames\n",
    "\n",
    "img_path = './images'\n",
    "split_images_files(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert LabelImg XML output to a csv file. so that we can use in fRCNN model annotation input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gereate_annotation_records():\n",
    "    # open csv file and create write object\n",
    "    annotated_file = open(\"annotated_data.csv\", 'w')\n",
    "    csvwriter = csv.writer(annotated_file)\n",
    "    csvwriter.writerow(['filename', 'fault_type', 'xmin', 'xmax', 'ymin', 'ymax'])\n",
    "\n",
    "    # loop over the xml files\n",
    "    xml_path = './annotations'\n",
    "    xml_files = os.listdir(xml_path)\n",
    "    for f in xml_files:\n",
    "        if f.endswith('xml'):\n",
    "            # create tree and root of xml file\n",
    "            tree = ET.parse(os.path.join(xml_path, f))\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # loop over the elements in the xml file\n",
    "            for element in root.findall('object'):\n",
    "                # get the input fields\n",
    "                name = element.find('name').text\n",
    "                bndbox = element.find('bndbox')\n",
    "                xmin = bndbox.find('xmin').text\n",
    "                xmax = bndbox.find('xmax').text\n",
    "                ymin = bndbox.find('ymin').text\n",
    "                ymax = bndbox.find('ymax').text\n",
    "\n",
    "                # create list with the values\n",
    "                row = []\n",
    "                new_path = f[:-4] + \".JPG\" # skip last four elements of f -> .xml extension\n",
    "                row.extend([new_path, name, xmin, xmax, ymin, ymax])\n",
    "\n",
    "                # write them away to csv\n",
    "                csvwriter.writerow(row)\n",
    "\n",
    "    # close csv file after appending the data\n",
    "    annotated_file.close()\n",
    "    # open the file\n",
    "    annotated_data = pd.read_csv('annotated_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to normalize defect types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_faulttype(ftype):\n",
    "    switcher={\n",
    "        'VG_PANEL':'VG_PANEL',\n",
    "        'VG PANEL':'VG_PANEL',\n",
    "        'VG _PANEL':'VG_PANEL',\n",
    "        'VG PANEL MISSING':'VG_MISSING_TEETH',\n",
    "        'VG_MISSING_TEETH':'VG_MISSING_TEETH',\n",
    "        'MISSING PANELS':'VG_MISSING_TEETH',\n",
    "        'VG _MISSING _TEETH':'VG_MISSING_TEETH',\n",
    "        'PANELS MISSING':'VG_MISSING_TEETH',\n",
    "        'LIGHTNING DENT':'SURFACE_DAMAGE_TY1',\n",
    "        'LIGHTNING _RECEPTOR':'SURFACE_DAMAGE_TY1',\n",
    "        'LIGHTNING_RECEPTOR':'SURFACE_DAMAGE_TY1',\n",
    "        'DENT':'SURFACE_DAMAGE_TY1',\n",
    "        'HIT BY SOME PARTICLES':'SURFACE_DAMAGE_TY1',\n",
    "        'LIGHTNING DAMAGE':'SURFACE_DAMAGE_TY1',\n",
    "        'SURFACE_DAMAGE_TY1':'SURFACE_DAMAGE_TY1'\n",
    "     }\n",
    "    return switcher.get(ftype,ftype)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = pd.read_csv('annotated_data.csv')\n",
    "annotated_data['fault_type'] = annotated_data.fault_type.apply(lambda x: x.upper() )\n",
    "annotated_data['fault_type'] = annotated_data.fault_type.apply(lambda x: change_faulttype(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VG_PANEL              280\n",
       "RUST                  279\n",
       "SURFACE_DAMAGE_TY1    277\n",
       "WIND EROSION          229\n",
       "VG_MISSING_TEETH      156\n",
       "MISSING                62\n",
       "BROKEN PART            47\n",
       "BROKEN                 21\n",
       "BROKEN PARTS           10\n",
       "CRACK                   3\n",
       "Name: fault_type, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_data.fault_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generated annotatipn file for faster-RCNN model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a9103e1b9e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_format'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./train_files'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JPG'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train_files'"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame(index=[-1],columns=['input_format'])\n",
    "train_file_path = './train_files'\n",
    "train_files = os.listdir(train_file_path)\n",
    "for f in train_files:\n",
    "    if str(f).find('JPG') > 0 :\n",
    "        image_fullpath= os.path.join(train_file_path,f)\n",
    "        if len(annotated_data[annotated_data.filename ==f]) > 0:\n",
    "            for _,row in annotated_data[annotated_data.filename ==f].iterrows():\n",
    "                if (row.fault_type in ['VG_MISSING_TEETH','VG_PANEL','SURFACE_DAMAGE_TY1']):\n",
    "              # add xmin, ymin, xmax, ymax and class as per the format required\n",
    "                    xmin = row.xmin\n",
    "                    xmax = row.xmax\n",
    "                    ymin = row.ymin\n",
    "                    ymax = row.ymax\n",
    "                    faulttype = row.fault_type\n",
    "                    data_str = image_fullpath + ',' + str(xmin) + ',' + str(ymin) + ',' + str(xmax) + ',' + str(ymax) + ',' + str(faulttype)\n",
    "                    train_data.loc[train_data.index.max()+1] = [data_str]\n",
    "        #else:\n",
    "            #print(f)\n",
    "            #data_str = image_fullpath + ',,,,,'\n",
    "            #train_data.loc[train_data.index.max()+1] = [data_str]\n",
    "train_data.drop([-1],inplace=True)\n",
    "train_data.to_csv('train_annotate1.txt', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =pd.read_csv('train_annotate10.txt', header=None, sep=',')\n",
    "train_data.columns =['imagename','x1','y1','x2','y2','defecttype']\n",
    "train_data[train_data.imagename.str.contains('0400.JPG')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making annotated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotated_image(image_path,imgname):\n",
    "    img=cv2.imread(imgname)\n",
    "    for _,row in train_data[train_data.imagename.str.contains(imgname)].iterrows():\n",
    "        print(row.defecttype)\n",
    "        real_x1, real_y1,real_x2, real_y2 = (int(row.x1),int(row.y1),int(row.x2),int(row.y2))\n",
    "        cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (36,255,12),1)\n",
    "        textLabel = row.defecttype\n",
    "        (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "        textOrg = (real_x1, real_y1-0)\n",
    "        cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n",
    "        cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "        cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "    cv2.imwrite(image_name1+'_annot3.jpg',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name1 = \"DJI_0212\"\n",
    "image_name ='./train_files/DJI_0212.JPG'\n",
    "make_annotated_image(image_name,image_name1):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
